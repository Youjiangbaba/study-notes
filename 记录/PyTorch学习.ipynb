{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、基本操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5., 7., 9.], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "obj = torch.Tensor(3,4)\n",
    "#1.判断是否是Tensor\n",
    "torch.is_tensor(obj) # isinstance(obj,torch.Tensor)\n",
    "\n",
    "#2.全局设置Tensor类型\n",
    "torch.set_default_tensor_type(torch.DoubleTensor) #torch.Tensor(2).dtype : torch.float64\n",
    "\n",
    "#3.获取Tensor元素个数\n",
    "torch.numel(obj)\n",
    "\n",
    "#4.与numpy互相转换\n",
    "def array2tensor(array):\n",
    "    return torch.from_numpy(array)\n",
    "def tensor2array(tensor):\n",
    "    return tensor.numpy()\n",
    "\n",
    "#5.创建tensor ，基本同numpy\n",
    "torch.zeros(2,2)\n",
    "torch.eye(3,3)#单位矩阵\n",
    "torch.arange(1,12,3)#step=3 1,4,7,10\n",
    "torch.linspace(1,9,5,requires_grad=True)# start/end/nums 等差数列\n",
    "torch.logspace(1,9,5,requires_grad=True)#10^1 - 10^9 nums = 5\n",
    "torch.ones(2,5)#元素为1的矩阵\n",
    "torch,rand(2,3)#均匀分布随机矩阵\n",
    "torch.randn(2,3)#标准正态分布随机矩阵\n",
    "torch.randperm(10)#numpy.random.permutation(10) 随机整数序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2、索引、切片、连接和换位**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([1, 2, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "#1.维度拼接\n",
    "ensor = torch.ones(2,3)\n",
    "#cat拼接，以dim进行拼接，dim = [0 , dims)\n",
    "print(torch.cat([ensor,ensor,ensor],dim = 0).size() )\n",
    "#stack拼接，叠加\n",
    "print(torch.stack([ensor,ensor],dim = 2).size() )\n",
    "\n",
    "#2.分片, chunk,按维度分片\n",
    "x = torch.ones(5,10)\n",
    "torch.chunk(x,5,dim=0)# 5个1*10; dim=2 五个2*2\n",
    "\n",
    "#3.按条件筛选出元素\n",
    "torch.index_select(x,1,torch.LongTensor([0,1]))#按索引返回元素,输入、维度、索引\n",
    "x = torch.rand(2,4)\n",
    "mask = x.ge(0.5) #返回x规格的bool类型的tensor，大于0.5为1，小于等于为0\n",
    "torch.masked_select(x,mask)#保留mask中为1的元素\n",
    "torch.nonzero(x) # == x.get(0)\n",
    "\n",
    "#4.维度改变\n",
    "print(x.shape)\n",
    "print (x.unsqueeze(dim=0).size())\n",
    "print (x.unsqueeze(dim=1).size())\n",
    "print (x.unsqueeze(dim=1).squeeze(dim=0).size())#去掉dim=0且其维度为1；不要参数，自动去除为1的维度\n",
    "\n",
    "#5.转置\n",
    "print( x.shape)\n",
    "print( torch.t(x).shape ) #x.t()\n",
    "print( torch.transpose(x,0,1).shape)#x.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3、元素数学计算**\n",
    "+-*/和基本四则运算一致，运算的矩阵的size要一致\n",
    "绝对值： torch.abs(x)\n",
    "求和： torch.add(t1,t2)\n",
    "余弦： torch.cos(x)\n",
    "反余弦： torch.acos(x)\n",
    "相乘再相加： torch.addcmul(a,0.1,t1,t2) = a + 0.1*t1*t2\n",
    "相除再相加： torch.addcdiv(a,0.1,t1,t2) = a + 0.1*t1/t2\n",
    "范围限制（夹逼函数）: torch.clamp(x, a,b) 大于a的设置为a，小于b的设置为b\n",
    "乘法: torch.mul(t1,t2) = t1*t2\n",
    "除法: torch.div(t1,t2) = t1/t2\n",
    "取相反数: torch.neg(x)\n",
    "取倒数: torch.reciprocal(x)\n",
    "取平方根: torch.sqrt()\n",
    "取余数: torch.fmod(x,3) = torch.remainder(x,3) = x%3\n",
    "\\\n",
    "取小数部分: torch.frac(x)\n",
    "四舍五入: torch.round(x)\n",
    "向上取整: torch.ceil(x)\n",
    "向下取整: torch.floor(x)\n",
    "\\\n",
    "e指数运算: torch.exp(x)\n",
    "e对数运算: torch.log(x)   还有 torch.log2(x) torch.log10(x)  \n",
    "幂运算: torch.pow(x,n)\n",
    "\\\n",
    "Sigmoid函数: torch.sigmoid(x)\n",
    "符号(sign)函数: torch.sign(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4、规约计算**\n",
    "计算元素的和: torch.sum(x,dim=0) ,去掉dim参数就是默认整体求\n",
    "计算元素的乘积: torch.prod(x)\n",
    "\n",
    "累加: torch.cumsum(x,dim=n)  返回相同size的Tensor，其中每个元素值为按方向下的累加结果\n",
    "累积: torch.cumprod(x,dim=n)\n",
    "\n",
    "p-norm距离，采用p范数：\n",
    "曼哈顿距离（1）: torch.dist(x,y,p=1)   \n",
    "欧式距离（2）： torch.dist(x,y,p=2)\n",
    "无穷范数： torch.dist(x,y,p=np.inf)\n",
    "p-norm范数: torch.norm(x,p=1,dim=0)\n",
    "\n",
    "均值: torch.mean(x)\n",
    "中位数: torch.median(x)  制定dim会返回两个Tensor，第一个是中位数，第二个是索引\n",
    "众数: torch.mode(x)\n",
    "方差: torch.var(x)\n",
    "标准差: torch.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2482, 0.8132, 0.5901, 0.0254],\n",
      "        [0.0543, 0.4632, 0.7734, 0.9272]])\n",
      "tensor([[0.2482, 1.0615, 1.6516, 1.6770],\n",
      "        [0.0543, 0.5175, 1.2909, 2.2181]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.8951)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "print(torch.cumsum(x,dim=1))#累加\n",
    "torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5、数值比较计算**\n",
    "比较元素是否相等: torch.eq(t1,t2)  返回bool-tensor\n",
    "比较元素是否大于后者: torch.gt(t1,t2)\n",
    "比较元素是否大于等于后者: torch.ge(t1,t2)\n",
    "比较Tensor是否相等: torch.equal(t1,t2) 返回true or False\n",
    "最大\\小值: torch.max(x) torch.min(x)\n",
    "\n",
    "排序: torch.sort(x,dim=0,descending=True) #降序，默认无参数为升序 ;返回两个Tensor，第一个为sorted，第二个为原始的\n",
    "返回最大/小的k个元素: torch.topk(x,k=2,largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([5., 3., 2., 2., 1., 0.]),\n",
       "indices=tensor([3, 2, 1, 5, 4, 0]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([0,2,3,5,1,2])\n",
    "torch.sort(a,dim=0,descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6、矩阵运算**\n",
    "x = torch.rand(3)\n",
    "对角矩阵: torch.diag(x, diagonal = 0)  diagonal对角线偏移量，0主，>0上偏，<0下偏\n",
    "下三角矩阵: torch.tril(x ,  diagonal = 0)\n",
    "上三角矩阵: torch.triu(x)\n",
    "矩阵的迹: torch.trace(x)\n",
    "\n",
    "矩阵乘积: torch.bmm(A,B)  torch.mm(A,B)   #矩阵A的列数需要等于B的行数\n",
    "两个一维向量的点积（元素相乘再相加）: torch.dot(x,y)\n",
    "\n",
    "批量相乘再相加: torch.addbmm(x,batch1,batch2,beta=0.1,alpha=10) #beta*x + b1.*b2*alpha\n",
    "矩阵乘向量再相加: torch.addmv(x,mat,vec,beta=0.1,alpha=10)   #beta*x + (mat.*vec)*alpha\n",
    "\n",
    "特征值及特征向量:\n",
    "torch.eig(x,eigenvectors=True) #eigenvectors为True，同时计算特征值与特征向量；否则只计算值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
